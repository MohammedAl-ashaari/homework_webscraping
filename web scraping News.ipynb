{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (4.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (3.0.5)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: jdcal in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from openpyxl) (1.4.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from openpyxl) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eng-moal-ashaari\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6070 sha256=ea55ef783dde61f6a09b625dd16cbab807c52d490a3f313d6c6a08f622255457\n",
      "  Stored in directory: c:\\users\\eng-moal-ashaari\\appdata\\local\\pip\\cache\\wheels\\83\\63\\2f\\117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "Successfully installed feedparser-6.0.12 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install feedparser requests beautifulsoup4 lxml pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ ุงููุญุงููุฉ ุงูุฃููู: RSS ...\n",
      "\n",
      "=== ุฃุญุฏุซ ุงูุฃุฎุจุงุฑ (ุฃูู 20) ===\n",
      "\n",
      "[1] ูุฏู ุงููููุบู ุงููุชุถุฑุฑุฉ ูู ุฅูุจููุง ุชุญุช ุงูุญุฌุฑ ุงูุตุญู ูุน ุงุฑุชูุงุน ุนุฏุฏ ุงูุฅุตุงุจุงุช\n",
      "ุชุงุฑูุฎ: 2025-09-10T23:14:00+03:00\n",
      "ูุณู: health\n",
      "ููุฎุต: ูุงู ูุณุคูููู ูุฐุง ุงูุฃุณุจูุน ุฅู ุงููุฏู ุงููุชุฃุซุฑุฉ ุจุฃุญุฏุซ ุชูุด ููุจุงุก ุฅูุจููุง ูู ุงููููุบู ุฃูุงูุช ููุงุท ุชูุชูุด ููุญุฏ ูู ุญุฑูุฉ ุงูุณูุงูุ ูุธุฑุง ูุงุฑุชูุงุน ุทููู ูู ุนุฏุฏ ุงูุฅุตุงุจุงุช ูู ููุช ุญุฐุฑ ููู ุนูุงู ุงูุฅุบุงุซุฉ ูู ููุต ุงูุชูููู.\n",
      "\n",
      "[2] ุชุญุฐูุฑ ูู ุงุฑุชูุงุน ูููุงุช ุงูููุงุฑูุง ูุน ุชุฑุงุฌุน ุงููุณุงุนุฏุงุช ุงูุฏูููุฉ\n",
      "ุชุงุฑูุฎ: 2025-09-10T23:11:00+03:00\n",
      "ูุณู: health\n",
      "ููุฎุต: ุญุฐุฑ ูุฏูุฑ ุงูุตูุฏูู ุงูุนุงููู ูููุงูุญุฉ ุงูุฃูุฑุงุถ ุงููุนุฏูุฉุ ุงูุฃุฑุจุนุงุกุ ูู ุงุญุชูุงู ุงุฑุชูุงุน ูููุงุช ุงูููุงุฑูุง ูุฐุง ุงูุนุงู ุจุณุจุจ ุชุฑุงุฌุน ุงููุณุงุนุฏุงุช ุงูุฏูููุฉ.\n",
      "\n",
      "[3] ุงูุนูู ุงูุฏูููุฉ: ุงูุชูุฌูุฑ ุงูุฌูุงุนู ูุณูุงู ุบุฒุฉ ุบูุฑ ูุงูููู ููุง ุฅูุณุงูู\n",
      "ุชุงุฑูุฎ: 2025-09-10T23:04:18+03:00\n",
      "ูุณู: news\n",
      "ููุฎุต: ุฏุนุช ููุธูุฉ ุงูุนูู ุงูุฏูููุฉ ุฅุณุฑุงุฆูู ุฅูู ุงูุฅูุบุงุก ุงูููุฑู ูุฃูุฑ ุงูุชูุฌูุฑ ุงูุฌูุงุนู ุงูุฐู ุฃุตุฏุฑู ุฌูุดูุง ูุณูุงู ูุฏููุฉ ุบุฒุฉ ูุน ุชุตุนูุฏ ูุฌูููุง ุนูู ุงููุฏููุฉุ ูุฃูุฏุช ุฃู ูุฐุง ุงูุฃูุฑ ููุงูู ูุนุงูุงุฉ ุงููุฏูููู ูุณุท ุฅุจุงุฏุฉ ุฌูุงุนูุฉ ูุณุชูุฑุฉ.\n",
      "\n",
      "[4] ุฃูุฑูุจุง ุชูุฏุฏ ุจุงุฎุชุฑุงู ูุณููุฑุงุช ุฃุฌูุงุก ุจูููุฏุง ูุฑูุณูุง ุชููู ูุณุคูููุชูุง\n",
      "ุชุงุฑูุฎ: 2025-09-10T22:27:33+03:00\n",
      "ูุณู: video\n",
      "ููุฎุต: ูุงู ุงูุฃููู ุงูุนุงู ูุญูู ุดูุงู ุงูุฃุทูุณู (ุงููุงุชู) ูุงุฑู ุฑูุชู ุฅู ุฑุฏ ุงูุญูู ุนูู ุงูุชูุงู ุงูุทุงุฆุฑุงุช ุงููุณููุฑุฉ ุงูุฑูุณูุฉ ููุฃุฌูุงุก ุงูุจูููุฏูุฉ ูุงุฌุญ ููุบุงูุฉุ ูุคูุฏุง ุฃู ุงููุงุชู ุฌุงูุฒ ููุฏูุงุน ุนู ูู ุดุจุฑ ูู ุฃุฑุงุถูู.\n",
      "\n",
      "[5] ููุชู ุงููุคุซุฑ ุงูุฃููุฑูู ุชุดุงุฑูู ููุฑู ูุชุฑุงูุจ ูุตูู ุจู\"ุงูุฃุณุทูุฑู ุงูุนุธูู\"\n",
      "ุชุงุฑูุฎ: 2025-09-10T22:22:03+03:00\n",
      "ูุณู: news\n",
      "ููุฎุต: ุฃุนูู ุงูุฑุฆูุณ ุงูุฃููุฑููุ ูุณุงุก ุฃูุณ ุงูุฃุฑุจุนุงุกุ ููุชู ุชุดุงุฑูู ููุฑู ุงููุงุดุท ูุงููุคุซุฑ ุนูู ููุงูุน ุงูุชูุงุตู ุงูุงุฌุชูุงุนู ุฅุซุฑ ุชุนุฑุถู ูุฅุทูุงู ุงููุงุฑ ุฎูุงู ูุดุงุฑูุชู ูู ูุนุงููุฉ ุจููุงูุฉ ููุชุง (ุบุฑุจู ุงูุจูุงุฏ).\n",
      "\n",
      "[6] ุฃุจุนุงุฏ ูุฏูุงูุงุช ุงููุตู ุงูุฅุณุฑุงุฆููู ุนูู ูุทุฑ\n",
      "ุชุงุฑูุฎ: 2025-09-10T22:15:51+03:00\n",
      "ูุณู: video\n",
      "ููุฎุต: ูุงูุด ุจุฑูุงูุฌ &quot;ูุณุงุฑ ุงูุฃุญุฏุงุซ&quot; ูู ุญููุฉ (2025/9/10) ุฃุจุนุงุฏ ูุฏูุงูุงุช ุงููุตู ุงูุฅุณุฑุงุฆููู ุนูู ูุทุฑ ูุชุฏุงุนูุงุชู ุนูู ุงูููุธููุฉ ุงูุฎููุฌูุฉุ ูุงูุฃูุฏุงู ุงูุชู ูุฑูู ุฅูููุง ุฑุฆูุณ ุงููุฒุฑุงุก ุงูุฅุณุฑุงุฆููู ูู ุงูููุทูุฉ.\n",
      "\n",
      "[7] ูุงุฐุง ุจุนุฏ ุงุชูุงู ุงุณุชุฆูุงู ุงูุชูุชูุด ุจูู ุฅูุฑุงู ูููุงูุฉ ุงูุทุงูุฉ ุงูุฐุฑูุฉุ\n",
      "ุชุงุฑูุฎ: 2025-09-10T22:15:01+03:00\n",
      "ูุณู: video\n",
      "ููุฎุต: ุจุฏุฃ ูุญุงูุธู ุงูููุงูุฉ ุงูุฏูููุฉ ููุทุงูุฉ ุงูุฐุฑูุฉ ูู ููููุง ููุงุดุงุชูู ุจุดุฃู ุงุชูุงู ุงุณุชุฆูุงู ุงูุชูุชูุด ุงูุฐู ููุน ุนููู ูู ุงููุงูุฑุฉ ุงููุฏูุฑ ุงูุนุงู ููููุงูุฉ ุฑุงูุงุฆูู ุบุฑูุณูุ ููุฒูุฑ ุงูุฎุงุฑุฌูุฉ ุงูุฅูุฑุงูู ุนุจุงุณ ุนุฑุงูุฌู.\n",
      "\n",
      "[8] ุงุญุชุฌุงุฌุงุช ูู ุจุงุฑูุณ ููุฏู ูุฑูุณูุฉ ุถุฏ ุณูุงุณุงุช ูุงูุฑูู ุงูุชูุดููุฉ\n",
      "ุชุงุฑูุฎ: 2025-09-10T22:13:01+03:00\n",
      "ูุณู: video\n",
      "ููุฎุต: ุดูุฏุช ูุฑูุณุง ูุธุงูุฑุงุช ูู ุงูุนุงุตูุฉ ุจุงุฑูุณ ููุฏู ุฃุฎุฑู ุฑูุถุง ููุชูุดู ูุงูุณูุงุณุงุช ุงูุงูุชุตุงุฏูุฉ ููุฑุฆูุณ ุฅููุงูููู ูุงูุฑูู ูุงูุญูููุงุช ุงููุชุนุงูุจุฉ.\n",
      "\n",
      "[9] ูู ูุฑูุฏ ูุชููุงูู ุฅุนุงุฏุฉ ุชุดููู ุงูุฎููุฌุ ููุง ุงููุทููุจ ูุฑุฏุนูุ\n",
      "ุชุงุฑูุฎ: 2025-09-10T21:43:31+03:00\n",
      "ูุณู: politics\n",
      "ููุฎุต: ุงุชูู ุฎุจุฑุงุก ูู ุงูุดุฃู ุงูุฎููุฌู ุนูู ุฃู ุงูุนุฏูุงู ุงูุฅุณุฑุงุฆููู ุนูู ูุทุฑ ูุณุชูุฏู ุฏูู ุงูุฎููุฌ ุจุฃุณุฑูุงุ ููุง ูุชุทูุจ ุถุฑูุฑุฉ ุงุชุฎุงุฐ ูุฑุงุฑุงุช ูุฅุฌุฑุงุก ูุฑุงุฌุนุงุช ูุฅุนุงุฏุฉ ุงููุธุฑ ูู ุงูุชุญุงููุงุช ุงููุงุฆูุฉ.\n",
      "\n",
      "[10] ุงููุงุจูุณ ุงูุฃูุจุฑ ูุฅุณุฑุงุฆูู ุงูุขู\n",
      "ุชุงุฑูุฎ: 2025-09-10T21:30:25+03:00\n",
      "ูุณู: opinions\n",
      "ููุฎุต: ููุงู ุญุชููุฉ ูุงูุชูุงุถุฉ ุฌุฏูุฏุฉุ ููููุง ุงุฎุชููุช ุงูุณููุงุฑูููุงุช ุงูุชูุตูููุฉ ูุดูู ุงูุงููุฌุงุฑ ุงูููุจูุ ูุฅู ุงูุซุงุจุช ุฃู ูุง ุจุนุฏู ูู ูููู ููุง ูุจููุ ูุณูููุชุจ ูุตูุง ุฌุฏูุฏุง ูู ุชุงุฑูุฎ ุงููุถูุฉ ุงูููุณุทูููุฉุ ูุฅู ุบุฏุง ููุงุธุฑู ูุฑูุจ.\n",
      "\n",
      "[11] ูุทุฑ ุชุคูุฏ ุงุณุชุดูุงุฏ ูุฏูุฑ ููุชุจ ุฎููู ุงูุญูุฉ ูู ุงููุฌูู ุงูุฅุณุฑุงุฆููู\n",
      "ุชุงุฑูุฎ: 2025-09-10T21:27:44+03:00\n",
      "ูุณู: news\n",
      "ููุฎุต: ุฃูุฏุช ูุฒุงุฑุฉ ุงูุฏุงุฎููุฉ ุงููุทุฑูุฉุ ูุณุงุก ุฃูุณ ุงูุฃุฑุจุนุงุกุ ููุชู ุฌูุงุฏ ุฑูุงุญ ุญุณู ูุจุฏุ ูุฏูุฑ ููุชุจ ุฎููู ุงูุญูุฉ ุฑุฆูุณ ุญุฑูุฉ ุญูุงุณ ูู ุบุฒุฉ ูุฑุฆูุณ ุงูููุฏ ุงูููุงูุถุ ุฅุซุฑ ูุฌูู ุฅุณุฑุงุฆููู ุงุณุชูุฏู ููุฑุงุช ุณูููุฉ ูููู ูููุง ูุงุฏุฉ ูู ุญูุงุณ.\n",
      "\n",
      "[12] ูุทุฑ: ูุณุชููุฑ ุชุตุฑูุญุงุช ูุชููุงูู ุงููุชููุฑุฉ ููุนูู ูุน ุดุฑูุงุฆูุง ูุถูุงู ูุญุงุณุจุชู\n",
      "ุชุงุฑูุฎ: 2025-09-10T21:17:20+03:00\n",
      "ูุณู: news\n",
      "ููุฎุต: ุงุณุชููุฑุช ุงูุฎุงุฑุฌูุฉ ุงููุทุฑูุฉ &quot;ุจุฃุดุฏ ุงูุนุจุงุฑุงุช&quot; ุงูุชุตุฑูุญุงุช &quot;ุงููุชููุฑุฉ&quot; ุงูุชู ุฃุฏูู ุจูุง ุฑุฆูุณ ุงููุฒุฑุงุก ุงูุฅุณุฑุงุฆููู ุจุดุฃู ุงุณุชุถุงูุฉ ุงูุฏูุญุฉ ููุชุจ ุญุฑูุฉ ุญูุงุณุ ูุนุจูุฑุช ุงููุฒุงุฑุฉ ุนู ุฅุฏุงูุชูุง ูุชูุฏูุฏุงุชู ุงูุตุฑูุญุฉ.\n",
      "\n",
      "[13] ุงูุญุฑุจ ุนูู ุบุฒุฉ ูุจุงุดุฑ.. ุนุดุฑุงุช ุงูุดูุฏุงุก ูุฅุฏุงูุงุช ูุชูุงุตูุฉ ูููุฌูู ุนูู ูุทุฑ\n",
      "ุชุงุฑูุฎ: 2025-09-10T21:09:40+03:00\n",
      "ูุณู: news\n",
      "ููุฎุต: ูู ุงูููู ุงูู706 ูู ุญุฑุจ ุงูุฅุจุงุฏุฉ ุนูู ุบุฒุฉุ ุฃูุงุฏุช ูุตุงุฏุฑ ูู ูุณุชุดููุงุช ุงููุทุงุน ุจุงุณุชุดูุงุฏ 72 ููุณุทูููุง ูู ุบุงุฑุงุช ุฅุณุฑุงุฆูููุฉ ุนูู ุงููุทุงุน ููุฐ ูุฌุฑ ุงูุฃุฑุจุนุงุกุ ุจูููู 53 ุจูุฏููุฉ ุบุฒุฉ.\n",
      "\n",
      "[14] ุดุงูุฏ.. ูู ุงูุชูุงุฏ ุงูุฌุฒุงุฆุฑููู ูููุฏุฑุจ ุจูุชููููุชุด ุญุงูุฉ ุตุญูุฉุ\n",
      "ุชุงุฑูุฎ: 2025-09-10T21:02:26+03:00\n",
      "ูุณู: video\n",
      "ููุฎุต: ุชุณูุฏ ุญุงูุฉ ูู ุงูุฌุฏู ุจุงูุฌุฒุงุฆุฑ ุญูู ูุตูุฑ ูุฏุฑุจ ููุชุฎุจ ูุฑุฉ ุงููุฏูุ ุงูุณููุณุฑู ุจูุชููููุชุดุ ุจูู ูู ููุถูู ุงุณุชูุฑุงุฑู ูู ุฃุฏุงุก ูููุชู ูุจูู ูู ูุทุงูุจ ุจุฅูุงูุชู.\n",
      "\n",
      "[15] ูุญูููู: ูุชููุงูู ูุถุน ุฃููุฑูุง ุจูุฃุฒู ููุง ูุณุชุทูุน ุชูุฑุงุฑ ุนุฏูุงูู ุนูู ูุทุฑ\n",
      "ุชุงุฑูุฎ: 2025-09-10T20:53:32+03:00\n",
      "ูุณู: news\n",
      "ููุฎุต: ูุนุชูุฏ ูุญูููู ุฃู ุฅุณุฑุงุฆูู ูู ูููู ุจููุฏูุฑูุง ุชูุฑุงุฑ ูุญุงููุชูุง ุงุณุชูุฏุงู ูุงุฏุฉ ุงูููุงููุฉ ุงูููุณุทูููุฉ ุงูููุฌูุฏูู ูู ุงูุนุงุตูุฉ ุงููุทุฑูุฉ ุงูุฏูุญุฉุ ูุฐูู ุฑุบู ุงูุชูุฏูุฏ ุงูุฐู ุฃุทููู ุจููุงููู ูุชููุงูู.\n",
      "\n",
      "[16] ุญูู ุชุตุจุญ ุงูููุงุฑุฉ ููุฏุง.. ุงููุฌู ุงููุธูู ููุฎุจุฑุฉ ูู ุณูู ุงูุนูู ุงูููู\n",
      "ุชุงุฑูุฎ: 2025-09-10T20:50:40+03:00\n",
      "ูุณู: misc\n",
      "ููุฎุต: ูู ุชุนุฑุถุช ูููุง ูุชุบููุฑุงุช ุดุงููุฉ ูู ุงูุนูู ุฃุซุฑุช ุนูู ููุงุฑุชู ูุจุงูุชุงูู ููุงูุชูุ ููุฑุจูุง ุชูุชูุฑ ูููุฑููุฉ ุงููุนุฑููุฉุ ููุง ููุ ูููู ุฃุตุจุญ ุงูุชููู ูุนูุงุฑุง ูููุฌุงุญ ูู ุณูู ุงูุนูู ุงูุญุฏูุซุ\n",
      "\n",
      "[17] ููู ุฃูุณุฏ ุชุบููุฑ ุงููููู ุตููุฉ ุงูุถูุงู ูุงููู ุฌูุงู ูุจุงูุฑู ููููุฎ\n",
      "ุชุงุฑูุฎ: 2025-09-10T20:29:20+03:00\n",
      "ูุณู: sport\n",
      "ููุฎุต: ูุดูุช ุตุญููุฉ &quot;ุจููุฏ&quot; ุงูุฃููุงููุฉุ ุฃู ูุงุฏู ุจุงูุฑู ููููุฎ ููุฑุฉ ุงููุฏู ุงูุชุฑุจ ูู ุงูุชุนุงูุฏ ูุน ุซูุงุฆู ุจุฑุดูููุฉ ูุงููู ุฌูุงู ูุบุงูู ูู ุนุงู 2022.\n",
      "\n",
      "[18] ูุชูู ูุฌุฑุญู ุฌุฑุงุก ูุตู \"ูุณุฏ\" ูุฑู ุจุฑูู ุญูุจ ุงูุดุฑูู\n",
      "ุชุงุฑูุฎ: 2025-09-10T20:20:00+03:00\n",
      "ูุณู: news\n",
      "ููุฎุต: ูุชู ูุฏูู ุณูุฑู ูุฃุตูุจ ุขุฎุฑูู ุจูููู ุทูู ุฌุฑุงุก ูุตู ููุงุช ุณูุฑูุง ุงูุฏูููุฑุงุทูุฉ (ูุณุฏ) ุจุฑุงุฌูุงุช ุงูุตูุงุฑูุฎ ูุฑู ุงูููุงุฑูุฉ (ุดุฑูู ูุฏููุฉ ุญูุจ ุดูุงู ุงูุจูุงุฏ) ูุงูุฎูุณุฉ ูุฑุณู ุงูุฃุญูุฑุ ููู ูุง ุฃูุงุฏุช ููุงุฉ ุงูุฅุฎุจุงุฑูุฉ ุงูุณูุฑูุฉ.\n",
      "\n",
      "[19] ุงูุนุฏูุงู ุนูู ูุทุฑ ูุดู ุบุฏุฑ ุฅุณุฑุงุฆูู ูุฑูุถูุง ููุณูุงู\n",
      "ุชุงุฑูุฎ: 2025-09-10T20:12:25+03:00\n",
      "ูุณู: video\n",
      "ููุฎุต: ูุดู ุงูุนุฏูุงู ุงูุฐู ุดูุชู ุฅุณุฑุงุฆูู ุนูู ุงูููุฏ ุงูููุงูุถ ูู ุญุฑูุฉ ุงูููุงููุฉ ุงูุฅุณูุงููุฉ (ุญูุงุณ)ุ ุญุฌู ุงูุบุฏุฑ ุงูุฅุณุฑุงุฆูููุ ูุฅุตุฑุงุฑ ูุฐู ุงูุฏููุฉ ุนูู ุงูุญุฑุจ ูุฑูุถูุง ููุณูุงู.\n",
      "\n",
      "[20] ุญุงุฑุณ ููุชุฎุจ ูุฑูุณุง ุงูุณุงุจู ูุงูุฏุงูุฏุง ูุนูู ุงุนุชุฒุงูู\n",
      "ุชุงุฑูุฎ: 2025-09-10T19:48:10+03:00\n",
      "ูุณู: sport\n",
      "ููุฎุต: ุฃุนูู ุญุงุฑุณ ุงููุฑูู ุงูุฏููู ุงูุณุงุจู ุณุชูู ูุงูุฏุงูุฏุงุ ุจุทู ุงูุนุงูู ูุน ุงูููุชุฎุจ ุงููุฑูุณู ููุฑุฉ ุงููุฏู ุนุงู 2018ุ ุนู ุงุนุชุฒุงูู ุงููุนุจ ููุงุฆูุงุ ููู ูู ุงูุฃุฑุจุนูู ูู ุนูุฑูุ ูู ููุงุจูุฉ ูุน ุตุญููุฉ &quot;ููููุจ&quot; ุงูุฑูุงุถูุฉ.\n",
      "ุชู ุฅูุดุงุก ุงูููู: aljazeera_ar_news.csv\n",
      "ุชู ุฅูุดุงุก ุงูููู: aljazeera_ar_news.json\n",
      "ุชู ุฅูุดุงุก ุงูููู: aljazeera_ar_news.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Al Jazeera Arabic news scraper (RSS + fallback HTML scraping)\n",
    "- ูุนุฑุถ ุงูุฃุฎุจุงุฑ ูู ุงูุทุฑููุฉ\n",
    "- ูุตุฏูุฑ ุฅูู CSV ู JSON (ู XLSX ุงุฎุชูุงุฑู)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ุญุงูู ุชุซุจูุช ูุฐู ุงูุญุฒู ุนูุฏ ุงูุญุงุฌุฉ:\n",
    "# pip install requests beautifulsoup4 lxml tqdm feedparser pandas openpyxl\n",
    "\n",
    "try:\n",
    "    import feedparser\n",
    "except ImportError:\n",
    "    feedparser = None\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    pd = None\n",
    "\n",
    "BASE = \"https://www.aljazeera.net\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/127.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ุจุนุถ RSS ุงูุดุงุฆุนุฉ (ูุฏ ุชุชุบูุฑ ูุน ุงูููุช)\n",
    "RSS_FEEDS = [\n",
    "    # ูู ุงูุฃุฎุจุงุฑ (ุฅู ูุฌุฏ)\n",
    "    \"https://www.aljazeera.net/rss\",\n",
    "    # ุงูุณูุงุณุฉ\n",
    "    \"https://www.aljazeera.net/politics/rss\",\n",
    "    # ุงูุชุตุงุฏ\n",
    "    \"https://www.aljazeera.net/economy/rss\",\n",
    "    # ุฑูุงุถุฉ\n",
    "    \"https://www.aljazeera.net/sports/rss\",\n",
    "    # ุชูุงุฑูุฑ\n",
    "    \"https://www.aljazeera.net/reports/rss\",\n",
    "]\n",
    "\n",
    "# ุงูุญุฏ ุงูุฃูุตู ููุฃุฎุจุงุฑ ุงูุชู ูุฑูุฏูุง\n",
    "MAX_ITEMS = 100\n",
    "\n",
    "def normalize_url(link: str) -> str:\n",
    "    if not link:\n",
    "        return \"\"\n",
    "    if link.startswith(\"//\"):\n",
    "        return \"https:\" + link\n",
    "    if link.startswith(\"/\"):\n",
    "        return urljoin(BASE, link)\n",
    "    return link\n",
    "\n",
    "def clean_text(x: str) -> str:\n",
    "    if not x:\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", x).strip()\n",
    "\n",
    "def try_parse_date(val: str):\n",
    "    for fmt in (\"%a, %d %b %Y %H:%M:%S %z\",\n",
    "                \"%Y-%m-%dT%H:%M:%S%z\",\n",
    "                \"%Y-%m-%dT%H:%M:%S%zZ\",\n",
    "                \"%Y-%m-%dT%H:%M:%S%zZ\",\n",
    "                \"%Y-%m-%dT%H:%M:%S%z\",\n",
    "                \"%Y-%m-%dT%H:%M:%S\",\n",
    "                \"%Y-%m-%d %H:%M:%S\",\n",
    "                \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(val, fmt)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def fetch(url):\n",
    "    r = requests.get(url, headers=HEADERS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "def parse_article_html(url: str) -> dict:\n",
    "    \"\"\"ูุญุงููุฉ ุงุณุชุฎุฑุงุฌ (ุงูุนููุงูุ ุงูููุฎุตุ ุงูุชุงุฑูุฎุ ุงููุณู) ูู ุตูุญุฉ ุฎุจุฑ ูุฑุฏูุฉ.\"\"\"\n",
    "    try:\n",
    "        r = fetch(url)\n",
    "    except Exception as e:\n",
    "        return {\"url\": url, \"error\": str(e)}\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # ุงูุนููุงู\n",
    "    title = \"\"\n",
    "    for sel in [\n",
    "        'meta[property=\"og:title\"]',\n",
    "        \"h1\", 'meta[name=\"twitter:title\"]'\n",
    "    ]:\n",
    "        node = soup.select_one(sel)\n",
    "        if node:\n",
    "            title = node.get(\"content\") if node.has_attr(\"content\") else node.get_text()\n",
    "            title = clean_text(title)\n",
    "            if title:\n",
    "                break\n",
    "\n",
    "    # ุงููุตู\n",
    "    summary = \"\"\n",
    "    for sel in [\n",
    "        'meta[name=\"description\"]',\n",
    "        'meta[property=\"og:description\"]',\n",
    "        'meta[name=\"twitter:description\"]'\n",
    "    ]:\n",
    "        node = soup.select_one(sel)\n",
    "        if node and node.get(\"content\"):\n",
    "            summary = clean_text(node[\"content\"])\n",
    "            if summary:\n",
    "                break\n",
    "\n",
    "    # ุงูุชุงุฑูุฎ\n",
    "    pub = \"\"\n",
    "    for sel in [\n",
    "        'meta[property=\"article:published_time\"]',\n",
    "        'meta[property=\"og:updated_time\"]',\n",
    "        'time[datetime]'\n",
    "    ]:\n",
    "        node = soup.select_one(sel)\n",
    "        if node:\n",
    "            pub = node.get(\"content\") or node.get(\"datetime\") or \"\"\n",
    "            pub = clean_text(pub)\n",
    "            if pub:\n",
    "                break\n",
    "    pub_dt = try_parse_date(pub) if pub else None\n",
    "    pub_str = pub_dt.isoformat() if pub_dt else pub\n",
    "\n",
    "    # ุงููุณู ูู ุงูุฑุงุจุท ุฃู ูุชุงุช ุงูุชููู\n",
    "    parsed = urlparse(url)\n",
    "    category = \"\"\n",
    "    parts = [p for p in parsed.path.split(\"/\") if p]\n",
    "    if parts:\n",
    "        # ูุซุงู: /politics/2025/9/10/slug\n",
    "        category = parts[0]\n",
    "\n",
    "    return {\n",
    "        \"title\": title or \"\",\n",
    "        \"summary\": summary or \"\",\n",
    "        \"published\": pub_str or \"\",\n",
    "        \"category\": category or \"\",\n",
    "        \"url\": url\n",
    "    }\n",
    "\n",
    "def scrape_homepage(max_links=50) -> list:\n",
    "    \"\"\"ุฎุทุฉ ุงุญุชูุงุทูุฉ: ูุฌูุน ุฑูุงุจุท ูู ุงูุตูุญุฉ ุงูุฑุฆูุณูุฉ ูุจุนุถ ุงูุตูุญุงุช ุงูุฏุงุฎููุฉ.\"\"\"\n",
    "    items = []\n",
    "    seen = set()\n",
    "\n",
    "    def collect_from(page_url):\n",
    "        try:\n",
    "            r = fetch(page_url)\n",
    "        except Exception:\n",
    "            return\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        # ุงูุชูุท ูู ุงูุฑูุงุจุท ุงูุฏุงุฎููุฉ ุฅูู ููุงูุงุช\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = normalize_url(a[\"href\"])\n",
    "            if not href.startswith(BASE):\n",
    "                continue\n",
    "            if any(x in href for x in [\"video\", \"live\", \"programs\"]):\n",
    "                continue\n",
    "            if re.search(r\"/\\d{4}/\\d{1,2}/\\d{1,2}/\", href):  # ูุญุชูู ุชุงุฑูุฎ ูู ุงูุฑุงุจุท\n",
    "                if href not in seen:\n",
    "                    seen.add(href)\n",
    "                    items.append(href)\n",
    "\n",
    "    # ุงุจุฏุฃ ุจุงูุตูุญุฉ ุงูุฑุฆูุณูุฉ ูุจุนุถ ุงูุฃูุณุงู ุงููุนุฑููุฉ\n",
    "    seeds = [\n",
    "        BASE,\n",
    "        f\"{BASE}/politics\",\n",
    "        f\"{BASE}/economy\",\n",
    "        f\"{BASE}/sports\",\n",
    "        f\"{BASE}/news\",\n",
    "        f\"{BASE}/reports\",\n",
    "        f\"{BASE}/opinions\",\n",
    "    ]\n",
    "    for s in seeds:\n",
    "        collect_from(s)\n",
    "        time.sleep(1)\n",
    "        if len(items) >= max_links:\n",
    "            break\n",
    "\n",
    "    items = items[:max_links]\n",
    "    results = []\n",
    "    for link in tqdm(items, desc=\"Parsing articles (HTML fallback)\"):\n",
    "        data = parse_article_html(link)\n",
    "        results.append(data)\n",
    "        time.sleep(0.8)  # ูุทูุงู ุจุงูุฎุงุฏู\n",
    "    return results\n",
    "\n",
    "def read_via_rss(max_items=MAX_ITEMS) -> list:\n",
    "    \"\"\"ูุญุงููุฉ ูุฑุงุกุฉ ุงูุฃุฎุจุงุฑ ุนุจุฑ RSS (ุงูุฃูุถู ุฅู ุชููุฑ).\"\"\"\n",
    "    if feedparser is None:\n",
    "        return []\n",
    "\n",
    "    collected = []\n",
    "    seen_links = set()\n",
    "    for feed in RSS_FEEDS:\n",
    "        try:\n",
    "            d = feedparser.parse(feed)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for e in d.entries:\n",
    "            link = normalize_url(getattr(e, \"link\", \"\"))\n",
    "            if not link or link in seen_links:\n",
    "                continue\n",
    "            seen_links.add(link)\n",
    "\n",
    "            title = clean_text(getattr(e, \"title\", \"\"))\n",
    "            summary = clean_text(getattr(e, \"summary\", \"\")) or clean_text(getattr(e, \"description\", \"\"))\n",
    "            published = \"\"\n",
    "            if getattr(e, \"published\", \"\"):\n",
    "                published = e.published\n",
    "            elif getattr(e, \"updated\", \"\"):\n",
    "                published = e.updated\n",
    "\n",
    "            # ุฌุฑูุจ ุชุญููู ุงูุชุงุฑูุฎ\n",
    "            pub_dt = try_parse_date(published) if published else None\n",
    "            pub_str = pub_dt.isoformat() if pub_dt else published\n",
    "\n",
    "            # ุงุณุชูุชุงุฌ ุงููุณู ูู ุงูุฑุงุจุท\n",
    "            parsed = urlparse(link)\n",
    "            parts = [p for p in parsed.path.split(\"/\") if p]\n",
    "            category = parts[0] if parts else \"\"\n",
    "\n",
    "            collected.append({\n",
    "                \"title\": title,\n",
    "                \"summary\": summary,\n",
    "                \"published\": pub_str,\n",
    "                \"category\": category,\n",
    "                \"url\": link\n",
    "            })\n",
    "            if len(collected) >= max_items:\n",
    "                break\n",
    "        if len(collected) >= max_items:\n",
    "            break\n",
    "\n",
    "    return collected\n",
    "\n",
    "def export_data(rows: list, basename=\"aljazeera_ar_news\"):\n",
    "    if not rows:\n",
    "        print(\"ูุง ุชูุฌุฏ ุจูุงูุงุช ูุชุตุฏูุฑูุง.\")\n",
    "        return\n",
    "\n",
    "    # CSV\n",
    "    csv_name = f\"{basename}.csv\"\n",
    "    with open(csv_name, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"title\", \"summary\", \"published\", \"category\", \"url\"])\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "    print(f\"ุชู ุฅูุดุงุก ุงูููู: {csv_name}\")\n",
    "\n",
    "    # JSON\n",
    "    json_name = f\"{basename}.json\"\n",
    "    with open(json_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rows, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ุชู ุฅูุดุงุก ุงูููู: {json_name}\")\n",
    "\n",
    "    # Excel (ุงุฎุชูุงุฑู)\n",
    "    if pd is not None:\n",
    "        try:\n",
    "            xlsx_name = f\"{basename}.xlsx\"\n",
    "            df = pd.DataFrame(rows)\n",
    "            df.to_excel(xlsx_name, index=False)\n",
    "            print(f\"ุชู ุฅูุดุงุก ุงูููู: {xlsx_name}\")\n",
    "        except Exception as e:\n",
    "            print(\"ุชุนุฐุฑ ุฅูุดุงุก Excel:\", e)\n",
    "    else:\n",
    "        print(\"ูุฅูุดุงุก ููู Excelุ ุซุจูุช pandas ู openpyxl.\")\n",
    "\n",
    "def main():\n",
    "    print(\"๐ ุงููุญุงููุฉ ุงูุฃููู: RSS ...\")\n",
    "    news = read_via_rss(max_items=MAX_ITEMS)\n",
    "\n",
    "    if not news:\n",
    "        print(\"๐ ูู ููุฌุญ RSSุ ุณูุญุงูู Web Scraping ุงูุงุญุชูุงุทู ูู ุงูุตูุญุงุช...\")\n",
    "        news = scrape_homepage(max_links=MAX_ITEMS)\n",
    "\n",
    "    # ุฅุฒุงูุฉ ุงููุฑุงุบุงุช ูุชุฑุชูุจ ุญุณุจ ุงูุชุงุฑูุฎ ุฅู ุฃููู\n",
    "    for n in news:\n",
    "        n[\"title\"] = clean_text(n.get(\"title\", \"\"))\n",
    "        n[\"summary\"] = clean_text(n.get(\"summary\", \"\"))\n",
    "        n[\"category\"] = clean_text(n.get(\"category\", \"\"))\n",
    "        n[\"url\"] = clean_text(n.get(\"url\", \"\"))\n",
    "\n",
    "    # ุทุจุงุนุฉ ุฃูู 20 ุฎุจุฑ ูููุนุงููุฉ\n",
    "    print(\"\\n=== ุฃุญุฏุซ ุงูุฃุฎุจุงุฑ (ุฃูู 20) ===\")\n",
    "    for i, item in enumerate(news[:20], 1):\n",
    "        print(f\"\\n[{i}] {item.get('title','')}\")\n",
    "        if item.get(\"published\"):\n",
    "            print(f\"ุชุงุฑูุฎ: {item['published']}\")\n",
    "        if item.get(\"category\"):\n",
    "            print(f\"ูุณู: {item['category']}\")\n",
    "        if item.get(\"summary\"):\n",
    "            print(f\"ููุฎุต: {item['summary'][:220]}{'...' if len(item['summary'])>220 else ''}\")\n",
    "\n",
    "    # ุชุตุฏูุฑ\n",
    "    export_data(news)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ุชุฐููุฑ ุฃุฎูุงูู:\n",
    "    # โ๏ธ ุฑุงุฌุน ุดุฑูุท ุงุณุชุฎุฏุงู ุงููููุน ู robots.txt\n",
    "    # โ๏ธ ุงุณุชุฎุฏู ุชุฃุฎูุฑุงู ุจูู ุงูุทูุจุงุช ููุง ุชุจุงูุบ ูู ุนุฏุฏูุง\n",
    "    # โ๏ธ ูุฐุง ุงูููุฏ ุชุนููููุ ูุฏ ููุฒู ุชุนุฏููู ุฅุฐุง ุชุบููุฑ ุจูุงุก ุตูุญุงุช ุงููููุน\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nุชู ุงูุฅููุงู ุจูุงุณุทุฉ ุงููุณุชุฎุฏู.\")\n",
    "    except Exception as e:\n",
    "        print(\"ุญุฏุซ ุฎุทุฃ:\", e, file=sys.stderr)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
